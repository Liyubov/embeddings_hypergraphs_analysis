{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb8-ZkSBwQ8p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distance learning with higher order constraints\n",
        "\n",
        "We want to consider the higher order case when we do metric learning algorithm.\n",
        "\n",
        "We do it as adaptation of the main notebook from https://github.com/kedemdor/metric-learning-talk\n",
        "\n",
        "## Distance learnign with classical binary constraints\n",
        "\n",
        "#### Loss functions\n",
        "\n",
        "In classification and regression problem loss functions are usually composed of a (often differential) transformation\n",
        "\n",
        "A metric learning loss function combines three elements:\n",
        "1. **The transformation function**: A differential transformation function $\\phi(x): \\mathbb{R}^n \\rightarrow \\mathbb{R}^d$ could either be a linear transformation, as we've seen in the Mahalanobis distance (i.e., $\\phi(x) = Lx$ where $L \\in \\mathbb{R}^{d\\times n}$), but it could also be a non-linear transformer (i.e. DNN or other feature ensembles) from an $n$-dimensional space to a $d$-dimensional space. The parameters of these functions w\n",
        "\n",
        "2. **The selected distance function to use in the latent space**: Most often, it would be either the Euclidean distance or the Cosine distance (which may be more meaningful in higher dimensional data).\n",
        "\n",
        "3. **The objective**: Using the provided supervision, the transformation function and the desired metric, we formalize an optimization problem. The specified supervision incorporated to a differentiable function to minimze or maximize.\n",
        "\n",
        "### Example loss function: linear LMNN for binary relations\n",
        "\n",
        "Let's take for example one of the more common metric learning loss function: Large Margin Nearest Neighbors (LMNN).\n",
        "loss function of LMNN is described as:\n",
        "\n",
        "$L\n",
        "=\n",
        "\\large\\min_\\limits{L}~~~\n",
        "\\lambda\\underbrace{\\sum_{i,~j\\in N_i} ||Lx_i-Lx_j||_2^2}_\\text{pull target neighbors} ~+(1-\\lambda)~\n",
        "\\underbrace{\\sum_{i,~j\\in N_i,~k\\notin N_i} max(0, 1 + ||Lx_i-Lx_j||_2^2 - ||Lx_i-Lx_k||_2^2)}_\\text{push away impostors from the neighborhood}$\n",
        "\n",
        "In this case:\n",
        "1. The transformation function is **linear** and defined by $L$: from $x\\in \\mathbb{R}^n$, to $Lx \\in \\mathbb{R}^d$.\n",
        "2. The distance function in the latent space is **Euclidean**, optimizing the loss function on the squared L2 distance: $d(\\vec{a},\\vec{b})^2 = ||\\vec{a}-\\vec{b}||_2^2$.\n",
        "3. The objective of this loss function is composed of two parts:\n",
        "    * A penalty for the squared distance between data points and their target neighbors (i.e. similar / within the same class), causing the optimization to ***pull*** them closer together in the latent space.\n",
        "    * A penalty for impostors (dissimilar / of different class) which are closer to the point than the target neighbors + a margin, causing the optimization to ***push*** them away from the neighborhood.\n",
        "    * The tradeoff between the two parts of the optimization is managed by the $\\lambda$ parameter, where $\\lambda=0.5$ is equal weight to the terms (best $\\lambda$ could be discovered by cross-validation).\n",
        "\n",
        "\n",
        "### Example loss function: linear LMNN for ternary relations\n",
        "\n",
        "Then we will just need to change the first constraint and introduce it into our method of Large Margin Nearest Neighbors (LMNN).\n",
        "loss function of LMNN is described below:\n",
        "\n",
        "$ L = \\large\\min_\\limits{L}~~~\n",
        "\\lambda\\underbrace{\\sum_{i,~j, k\\in N_i} ||L(x_i,x_j, x_k)||_2^2}_\\text{pull target neighbors} ~+(1-\\lambda)~\n",
        "\\underbrace{\\sum_{i,~j\\in N_i,~k\\notin N_i} max(0, 1 + ||Lx_i-Lx_j||_2^2 - ||Lx_i-Lx_k||_2^2)}_\\text{push away impostors from the neighborhood}$\n"
      ],
      "metadata": {
        "id": "PLtYvPfqwnfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this demo, we would use these packages\n",
        "!pip install -q huggingface-hub==0.0.19\n",
        "!pip install -q scikit-learn==1.0 delayed\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q pytorch-metric-learning[with-hooks]\n",
        "!pip install -q umap-learn\n",
        "!pip install -q umap-learn[plot]\n",
        "!pip install -q metric-learn\n",
        "!pip install -q altair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDR8c4qFzwOf",
        "outputId": "70b5fa64-2ee6-466d-9b63-61b39f80b20e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "accelerate 1.1.1 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.0.19 which is incompatible.\n",
            "diffusers 0.31.0 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.0.19 which is incompatible.\n",
            "peft 0.13.2 requires huggingface-hub>=0.17.0, but you have huggingface-hub 0.0.19 which is incompatible.\n",
            "sentence-transformers 3.2.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.0.19 which is incompatible.\n",
            "tokenizers 0.20.3 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.0.19 which is incompatible.\n",
            "transformers 4.46.2 requires huggingface-hub<1.0,>=0.23.2, but you have huggingface-hub 0.0.19 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample data preparation\n",
        "Importing stanadrd data of texts.\n",
        "1. Step 1: Sentence embedding. To get from the textual representation of the customer query to a vectorized representation, we will use some state-of-the-art BERT setentence embeddings. These general-domain transformers are used to project a textual sentence (usually up to 128 words) into a 300-768 dimension embedding which should preserve their semantic meaning. This would be used as our initial input for metric learning, where we would learn a bank-specific representation.We will use\n",
        "\n",
        "* The Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks package.\n",
        "* For this we will need\n",
        "NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
      ],
      "metadata": {
        "id": "lnCw3yKE-dQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def read_dataset(set_name: str, show_samples:int=0 , random_state:int=42):\n",
        "    # Retrieve the banking data from the github repository.\n",
        "    url = f\"https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/{set_name}.csv\"\n",
        "    dataset = pd.read_csv(url)\n",
        "    # Describe the dataset.\n",
        "    print(f\"There's a total of [{len(dataset)}] texts \" +\n",
        "          f\"across [{len(set(dataset['category']))}] categories in the [{set_name}] set.\\n\")\n",
        "    # Show an example of the dataset.\n",
        "    if show_samples:\n",
        "        print(\"Here are some examples:\\n------------------------\")\n",
        "        for _, row in dataset.sample(show_samples, random_state=random_state).iterrows():\n",
        "            print(f\"category: {row.category}\\ntext: {row.text}\\n\")\n",
        "    # Return the read dataset.\n",
        "    return dataset\n",
        "\n",
        "train_df = read_dataset(\"train\", show_samples=4)\n",
        "test_df = read_dataset(\"test\", show_samples=4)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Sample a number of sentences per category.\n",
        "def sample_and_aggregate_dataset(df:pd.DataFrame, samples_per_category:int=20,\n",
        "                                 random_state:int=42, verbose:bool=False):\n",
        "  \"\"\" Aggregating on category, sampling a number of queries per category. \"\"\"\n",
        "  random.seed(random_state)\n",
        "  agg_df = pd.DataFrame(\n",
        "    df.groupby(\"category\").agg(\n",
        "        lambda corpus: random.sample(list(corpus), min([samples_per_category, len(corpus)])))\n",
        "      .to_records())\n",
        "  return agg_df\n",
        "\n",
        "train_agg_df = sample_and_aggregate_dataset(train_df, samples_per_category=10, verbose=True)\n",
        "test_agg_df = sample_and_aggregate_dataset(test_df, samples_per_category=2000, verbose=True)\n",
        "\n",
        "print(f\"Total number of points in train set reduced to [{sum(train_agg_df.text.map(len))}].\")\n",
        "print(f\"Total number of points in test set reduced to [{sum(test_agg_df.text.map(len))}].\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASY18DTC6B1t",
        "outputId": "93de9e43-0c42-45de-b04f-e247a0cb571b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There's a total of [10003] texts across [77] categories in the [train] set.\n",
            "\n",
            "Here are some examples:\n",
            "------------------------\n",
            "category: change_pin\n",
            "text: Is it possible for me to change my PIN number?\n",
            "\n",
            "category: declined_card_payment\n",
            "text: I'm not sure why my card didn't work\n",
            "\n",
            "category: top_up_failed\n",
            "text: I don't think my top up worked\n",
            "\n",
            "category: card_payment_fee_charged\n",
            "text: Can you explain why my payment was charged a fee?\n",
            "\n",
            "There's a total of [3080] texts across [77] categories in the [test] set.\n",
            "\n",
            "Here are some examples:\n",
            "------------------------\n",
            "category: card_linking\n",
            "text: How do I link this new card?\n",
            "\n",
            "category: card_swallowed\n",
            "text: How do I retrieve my card from the machine?\n",
            "\n",
            "category: verify_source_of_funds\n",
            "text: I want to know where the funds come from.\n",
            "\n",
            "category: automatic_top_up\n",
            "text: I just activated auto top-up, but it is not letting me enable it. Why not?\n",
            "\n",
            "Total number of points in train set reduced to [770].\n",
            "Total number of points in test set reduced to [3080].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Step 1: Sentence embedding.\n",
        "\n",
        "\n",
        "device = \"cuda\"  # Change to \"cpu\" if running on CPU or \"cuda\" if you run with GPU.\n",
        "# Loading the pretrained sentence BERT model.\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# STSB = Sentence Transformer Siamese BERT.\n",
        "# Note: Recently I've received error that the download sometimes fails.\n",
        "# If that happens, just make sure you're outside of the VPN & retry.\n",
        "sbert_model = SentenceTransformer('stsb-mpnet-base-v2', device=device)"
      ],
      "metadata": {
        "id": "sZhDCG7Gb3Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed the samples in with the sentence2vec transformer. May take a 4-8 minutes if on CPU...\n",
        "train_agg_df[\"sentence2vec\"] = train_agg_df[\"text\"].map(sbert_model.encode)\n",
        "test_agg_df[\"sentence2vec\"] = test_agg_df[\"text\"].map(sbert_model.encode)"
      ],
      "metadata": {
        "id": "oU_FirjkEWkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.  Evaluating the embedding\n",
        "\n",
        "Before we begin with the metric learning, let's first evaluate our initial general-purpose embeddings. There are different ways of measuring embeddings, but a common practice is to look into the neighborhoods and see if the nearby neighbors are semantically similar. Throughout the notebook, we will use three ways to evaluate the embedding.\n",
        "For now we can skip it...\n"
      ],
      "metadata": {
        "id": "pywQAHD_cBQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "embeddings_tsne = TSNE(random_state=42, init='pca', learning_rate='auto').fit_transform(X=np.concatenate([X_train,X_test]))\n",
        "vis_df[\"embeddings_tsne_0\"] = embeddings_tsne[:,0]\n",
        "vis_df[\"embeddings_tsne_1\"] = embeddings_tsne[:,1]\n",
        "\n",
        "render_embedding(vis_df, title=\"query =[STSB]=> 768 =[TSNE]=> 2 (Better!)\",\n",
        "                 coord_0_col='embeddings_tsne_0',\n",
        "                 coord_1_col='embeddings_tsne_1')"
      ],
      "metadata": {
        "id": "PDW6C2JGcHIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.1: Linear metric learning\n",
        "We will use LMNN, but feel free to [explore the package](http://contrib.scikit-learn.org/metric-learn/metric_learn.html#supervised-learning-algorithms) and try other types of linear transformations, such as NCA or ITML.\n",
        "\n",
        "Also, we're reducing the number of components in the new representation from 768 to 128, for two main reasons. First, it's faster to learn a transformation matrix of size (768 x 128) than it is of size (768 x 768). But more importantly, the core reasoning behind using a lower dimension is that if we used 768 features to represent the variance of a general purpose sentence, you don't need that many feaures to differentiate between the 77 categories of banking queries. And third, because we a small number of data points, using a smaller dimension in the latent space helps us avoid overfitting.\n",
        "\n",
        "\n",
        "\n",
        "We train the LMNN transformer with `verbose=True` so you could follow up on what happens. The will see two interesting details:\n",
        "* The **objective**, which is the same objective function described above, which with gradient descend we will attempt to lower every iteration.\n",
        "* The **active constraints**, which are how many \"impostors\" are there inside neighborhoods they shouldn't be a part of. The lower this number is, the more \"pure\" and homogeneous the neighbors after the transformation.\n",
        "\n",
        "When you attempt to beat this baseline, try different values for `n_components` (number of dimensions in the output representation), `k` (how many neighbors should we take into account when searching for impostors) and `regularization` which determines the ratio between pushing and pulling.\n",
        "\n",
        "If your attempt doesn't converge, try increasing the `learn_rate` or increase the `max_iter`.\n"
      ],
      "metadata": {
        "id": "ohDD98x5cGfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from metric_learn import LMNN\n",
        "lmnn = LMNN(n_components=192, k=5,\n",
        "            learn_rate=1e-2, max_iter=2000, random_state=42,\n",
        "            verbose=True).fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "iU652EKRb2LN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Changing active constraints in the function\n",
        "\n",
        "Now we want to change this and write another LMNN function in this library\n",
        "K. Q. Weinberger, J. Blitzer, L. K. Saul. Distance Metric Learning for Large Margin Nearest Neighbor Classification. NIPS 2005.\n",
        "http://contrib.scikit-learn.org/metric-learn/generated/metric_learn.LMNN.html\n",
        "\n",
        "\n",
        "So now we can use the constraint of higher order in the data and use the specific function on optimisation:\n",
        "\n",
        "See more details in the simulation above\n",
        "**iter | objective | objective difference | active constraints | learning rate**\n",
        "\n",
        "\n",
        "We will look into the source code of the standard LMNN_classic\n",
        "and will add LMNN_higher_orer\n"
      ],
      "metadata": {
        "id": "8-EIDCoMc36-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMNN_classic(MahalanobisMixin, TransformerMixin):\n",
        "  \"\"\"Large Margin Nearest Neighbor (LMNN)\n",
        "\n",
        "  LMNN learns a Mahalanobis distance metric in the kNN classification\n",
        "  setting. The learned metric attempts to keep close k-nearest neighbors\n",
        "  from the same class, while keeping examples from different classes\n",
        "  separated by a large margin. This algorithm makes no assumptions about\n",
        "  the distribution of the data.\n",
        "\n",
        "  Read more in the :ref:`User Guide <lmnn>`.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  init : string or numpy array, optional (default='auto')\n",
        "    Initialization of the linear transformation. Possible options are\n",
        "    'auto', 'pca', 'identity', 'random', and a numpy array of shape\n",
        "    (n_features_a, n_features_b).\n",
        "\n",
        "    'auto'\n",
        "      Depending on ``n_components``, the most reasonable initialization\n",
        "      will be chosen. If ``n_components <= n_classes`` we use 'lda', as\n",
        "      it uses labels information. If not, but\n",
        "      ``n_components < min(n_features, n_samples)``, we use 'pca', as\n",
        "      it projects data in meaningful directions (those of higher\n",
        "      variance). Otherwise, we just use 'identity'.\n",
        "\n",
        "    'pca'\n",
        "      ``n_components`` principal components of the inputs passed\n",
        "      to :meth:`fit` will be used to initialize the transformation.\n",
        "      (See `sklearn.decomposition.PCA`)\n",
        "\n",
        "    'lda'\n",
        "      ``min(n_components, n_classes)`` most discriminative\n",
        "      components of the inputs passed to :meth:`fit` will be used to\n",
        "      initialize the transformation. (If ``n_components > n_classes``,\n",
        "      the rest of the components will be zero.) (See\n",
        "      `sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)\n",
        "\n",
        "    'identity'\n",
        "      If ``n_components`` is strictly smaller than the\n",
        "      dimensionality of the inputs passed to :meth:`fit`, the identity\n",
        "      matrix will be truncated to the first ``n_components`` rows.\n",
        "\n",
        "    'random'\n",
        "      The initial transformation will be a random array of shape\n",
        "      `(n_components, n_features)`. Each value is sampled from the\n",
        "      standard normal distribution.\n",
        "\n",
        "    numpy array\n",
        "      n_features_b must match the dimensionality of the inputs passed to\n",
        "      :meth:`fit` and n_features_a must be less than or equal to that.\n",
        "      If ``n_components`` is not None, n_features_a must match it.\n",
        "\n",
        "  n_neighbors : int, optional (default=3)\n",
        "    Number of neighbors to consider, not including self-edges.\n",
        "\n",
        "  min_iter : int, optional (default=50)\n",
        "    Minimum number of iterations of the optimization procedure.\n",
        "\n",
        "  max_iter : int, optional (default=1000)\n",
        "    Maximum number of iterations of the optimization procedure.\n",
        "\n",
        "  learn_rate : float, optional (default=1e-7)\n",
        "    Learning rate of the optimization procedure\n",
        "\n",
        "  tol : float, optional (default=0.001)\n",
        "    Tolerance of the optimization procedure. If the objective value varies\n",
        "    less than `tol`, we consider the algorithm has converged and stop it.\n",
        "\n",
        "  verbose : bool, optional (default=False)\n",
        "    Whether to print the progress of the optimization procedure.\n",
        "\n",
        "  regularization: float, optional (default=0.5)\n",
        "    Relative weight between pull and push terms, with 0.5 meaning equal\n",
        "    weight.\n",
        "\n",
        "  preprocessor : array-like, shape=(n_samples, n_features) or callable\n",
        "    The preprocessor to call to get tuples from indices. If array-like,\n",
        "    tuples will be formed like this: X[indices].\n",
        "\n",
        "  n_components : int or None, optional (default=None)\n",
        "    Dimensionality of reduced space (if None, defaults to dimension of X).\n",
        "\n",
        "  random_state : int or numpy.RandomState or None, optional (default=None)\n",
        "    A pseudo random number generator object or a seed for it if int. If\n",
        "    ``init='random'``, ``random_state`` is used to initialize the random\n",
        "    transformation. If ``init='pca'``, ``random_state`` is passed as an\n",
        "    argument to PCA when initializing the transformation.\n",
        "\n",
        "  k : Renamed to n_neighbors. Will be deprecated in 0.7.0\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  n_iter_ : `int`\n",
        "    The number of iterations the solver has run.\n",
        "\n",
        "  components_ : `numpy.ndarray`, shape=(n_components, n_features)\n",
        "    The learned linear transformation ``L``.\n",
        "\n",
        "  Examples\n",
        "  --------\n",
        "\n",
        "  >>> import numpy as np\n",
        "  >>> from metric_learn import LMNN\n",
        "  >>> from sklearn.datasets import load_iris\n",
        "  >>> iris_data = load_iris()\n",
        "  >>> X = iris_data['data']\n",
        "  >>> Y = iris_data['target']\n",
        "  >>> lmnn = LMNN(n_neighbors=5, learn_rate=1e-6)\n",
        "  >>> lmnn.fit(X, Y, verbose=False)\n",
        "\n",
        "  References\n",
        "  ----------\n",
        "  .. [1] K. Q. Weinberger, J. Blitzer, L. K. Saul. `Distance Metric\n",
        "         Learning for Large Margin Nearest Neighbor Classification\n",
        "         <http://papers.nips.cc/paper/2795-distance-metric\\\n",
        "         -learning-for-large-margin-nearest-neighbor-classification>`_. NIPS\n",
        "         2005.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "[docs]\n",
        "\n",
        "  def __init__(self, init='auto', n_neighbors=3, min_iter=50, max_iter=1000,\n",
        "               learn_rate=1e-7, regularization=0.5, convergence_tol=0.001,\n",
        "               verbose=False, preprocessor=None,\n",
        "               n_components=None, random_state=None, k='deprecated'):\n",
        "    self.init = init\n",
        "    if k != 'deprecated':\n",
        "      warnings.warn('\"num_chunks\" parameter has been renamed to'\n",
        "                    ' \"n_chunks\". It has been deprecated in'\n",
        "                    ' version 0.6.3 and will be removed in 0.7.0'\n",
        "                    '', FutureWarning)\n",
        "      n_neighbors = k\n",
        "    self.k = 'deprecated'  # To avoid no_attribute error\n",
        "    self.n_neighbors = n_neighbors\n",
        "    self.min_iter = min_iter\n",
        "    self.max_iter = max_iter\n",
        "    self.learn_rate = learn_rate\n",
        "    self.regularization = regularization\n",
        "    self.convergence_tol = convergence_tol\n",
        "    self.verbose = verbose\n",
        "    self.n_components = n_components\n",
        "    self.random_state = random_state\n",
        "    super(LMNN, self).__init__(preprocessor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[docs]\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    k = self.n_neighbors\n",
        "    reg = self.regularization\n",
        "    learn_rate = self.learn_rate\n",
        "\n",
        "    X, y = self._prepare_inputs(X, y, dtype=float,\n",
        "                                ensure_min_samples=2)\n",
        "    num_pts, d = X.shape\n",
        "    output_dim = _check_n_components(d, self.n_components)\n",
        "    unique_labels, label_inds = np.unique(y, return_inverse=True)\n",
        "    if len(label_inds) != num_pts:\n",
        "      raise ValueError('Must have one label per point.')\n",
        "    self.labels_ = np.arange(len(unique_labels))\n",
        "\n",
        "    self.components_ = _initialize_components(output_dim, X, y, self.init,\n",
        "                                              self.verbose,\n",
        "                                              random_state=self.random_state)\n",
        "    required_k = np.bincount(label_inds).min()\n",
        "    if self.n_neighbors > required_k:\n",
        "      raise ValueError('not enough class labels for specified k'\n",
        "                       ' (smallest class has %d)' % required_k)\n",
        "\n",
        "    target_neighbors = self._select_targets(X, label_inds)\n",
        "\n",
        "    # sum outer products\n",
        "    dfG = _sum_outer_products(X, target_neighbors.flatten(),\n",
        "                              np.repeat(np.arange(X.shape[0]), k))\n",
        "\n",
        "    # initialize L\n",
        "    L = self.components_\n",
        "\n",
        "    # first iteration: we compute variables (including objective and gradient)\n",
        "    #  at initialization point\n",
        "    G, objective, total_active = self._loss_grad(X, L, dfG, k,\n",
        "                                                 reg, target_neighbors,\n",
        "                                                 label_inds)\n",
        "\n",
        "    it = 1  # we already made one iteration\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"iter | objective | objective difference | active constraints\",\n",
        "            \"| learning rate\")\n",
        "\n",
        "    # main loop\n",
        "    for it in range(2, self.max_iter):\n",
        "      # then at each iteration, we try to find a value of L that has better\n",
        "      # objective than the previous L, following the gradient:\n",
        "      while True:\n",
        "        # the next point next_L to try out is found by a gradient step\n",
        "        L_next = L - learn_rate * G\n",
        "        # we compute the objective at next point\n",
        "        # we copy variables that can be modified by _loss_grad, because if we\n",
        "        # retry we don t want to modify them several times\n",
        "        (G_next, objective_next, total_active_next) = (\n",
        "            self._loss_grad(X, L_next, dfG, k, reg, target_neighbors,\n",
        "                            label_inds))\n",
        "        assert not np.isnan(objective)\n",
        "        delta_obj = objective_next - objective\n",
        "        if delta_obj > 0:\n",
        "          # if we did not find a better objective, we retry with an L closer to\n",
        "          # the starting point, by decreasing the learning rate (making the\n",
        "          # gradient step smaller)\n",
        "          learn_rate /= 2\n",
        "        else:\n",
        "          # otherwise, if we indeed found a better obj, we get out of the loop\n",
        "          break\n",
        "      # when the better L is found (and the related variables), we set the\n",
        "      # old variables to these new ones before next iteration and we\n",
        "      # slightly increase the learning rate\n",
        "      L = L_next\n",
        "      G, objective, total_active = G_next, objective_next, total_active_next\n",
        "      learn_rate *= 1.01\n",
        "\n",
        "      if self.verbose:\n",
        "        print(it, objective, delta_obj, total_active, learn_rate)\n",
        "\n",
        "      # check for convergence\n",
        "      if it > self.min_iter and abs(delta_obj) < self.convergence_tol:\n",
        "        if self.verbose:\n",
        "          print(\"LMNN converged with objective\", objective)\n",
        "        break\n",
        "    else:\n",
        "      if self.verbose:\n",
        "        print(\"LMNN didn't converge in %d steps.\" % self.max_iter)\n",
        "\n",
        "    # store the last L\n",
        "    self.components_ = L\n",
        "    self.n_iter_ = it\n",
        "    return self\n",
        "\n",
        "\n",
        "\n",
        "  def _loss_grad(self, X, L, dfG, k, reg, target_neighbors, label_inds):\n",
        "    # Compute pairwise distances under current metric\n",
        "    Lx = L.dot(X.T).T\n",
        "\n",
        "    # we need to find the furthest neighbor:\n",
        "    Ni = 1 + _inplace_paired_L2(Lx[target_neighbors], Lx[:, None, :])\n",
        "    furthest_neighbors = np.take_along_axis(target_neighbors,\n",
        "                                            Ni.argmax(axis=1)[:, None], 1)\n",
        "    impostors = self._find_impostors(furthest_neighbors.ravel(), X,\n",
        "                                     label_inds, L)\n",
        "\n",
        "    g0 = _inplace_paired_L2(*Lx[impostors])\n",
        "\n",
        "    # we reorder the target neighbors\n",
        "    g1, g2 = Ni[impostors]\n",
        "    # compute the gradient\n",
        "    total_active = 0\n",
        "    df = np.zeros((X.shape[1], X.shape[1]))\n",
        "    for nn_idx in reversed(range(k)):  # note: reverse not useful here\n",
        "      act1 = g0 < g1[:, nn_idx]\n",
        "      act2 = g0 < g2[:, nn_idx]\n",
        "      total_active += act1.sum() + act2.sum()\n",
        "\n",
        "      targets = target_neighbors[:, nn_idx]\n",
        "      PLUS, pweight = _count_edges(act1, act2, impostors, targets)\n",
        "      df += _sum_outer_products(X, PLUS[:, 0], PLUS[:, 1], pweight)\n",
        "\n",
        "      in_imp, out_imp = impostors\n",
        "      df -= _sum_outer_products(X, in_imp[act1], out_imp[act1])\n",
        "      df -= _sum_outer_products(X, in_imp[act2], out_imp[act2])\n",
        "\n",
        "    # do the gradient update\n",
        "    assert not np.isnan(df).any()\n",
        "    G = dfG * reg + df * (1 - reg)\n",
        "    G = L.dot(G)\n",
        "    # compute the objective function\n",
        "    objective = total_active * (1 - reg)\n",
        "    objective += G.flatten().dot(L.flatten())\n",
        "    return 2 * G, objective, total_active\n",
        "\n",
        "  def _select_targets(self, X, label_inds):\n",
        "    target_neighbors = np.empty((X.shape[0], self.n_neighbors), dtype=int)\n",
        "    for label in self.labels_:\n",
        "      inds, = np.nonzero(label_inds == label)\n",
        "      dd = euclidean_distances(X[inds], squared=True)\n",
        "      np.fill_diagonal(dd, np.inf)\n",
        "      nn = np.argsort(dd)[..., :self.n_neighbors]\n",
        "      target_neighbors[inds] = inds[nn]\n",
        "    return target_neighbors\n",
        "\n",
        "  def _find_impostors(self, furthest_neighbors, X, label_inds, L):\n",
        "    Lx = X.dot(L.T)\n",
        "    margin_radii = 1 + _inplace_paired_L2(Lx[furthest_neighbors], Lx)\n",
        "    impostors = []\n",
        "    for label in self.labels_[:-1]:\n",
        "      in_inds, = np.nonzero(label_inds == label)\n",
        "      out_inds, = np.nonzero(label_inds > label)\n",
        "      dist = euclidean_distances(Lx[out_inds], Lx[in_inds], squared=True)\n",
        "      i1, j1 = np.nonzero(dist < margin_radii[out_inds][:, None])\n",
        "      i2, j2 = np.nonzero(dist < margin_radii[in_inds])\n",
        "      i = np.hstack((i1, i2))\n",
        "      j = np.hstack((j1, j2))\n",
        "      if i.size > 0:\n",
        "        # get unique (i,j) pairs using index trickery\n",
        "        shape = (i.max() + 1, j.max() + 1)\n",
        "        tmp = np.ravel_multi_index((i, j), shape)\n",
        "        i, j = np.unravel_index(np.unique(tmp), shape)\n",
        "      impostors.append(np.vstack((in_inds[j], out_inds[i])))\n",
        "    if len(impostors) == 0:\n",
        "        # No impostors detected\n",
        "        return impostors\n",
        "    return np.hstack(impostors)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _inplace_paired_L2(A, B):\n",
        "  '''Equivalent to ((A-B)**2).sum(axis=-1), but modifies A in place.'''\n",
        "  A -= B\n",
        "  return np.einsum('...ij,...ij->...i', A, A)\n",
        "\n",
        "\n",
        "def _count_edges(act1, act2, impostors, targets):\n",
        "  imp = impostors[0, act1]\n",
        "  c = Counter(zip(imp, targets[imp]))\n",
        "  imp = impostors[1, act2]\n",
        "  c.update(zip(imp, targets[imp]))\n",
        "  if c:\n",
        "    active_pairs = np.array(list(c.keys()))\n",
        "  else:\n",
        "    active_pairs = np.empty((0, 2), dtype=int)\n",
        "  return active_pairs, np.array(list(c.values()))\n",
        "\n",
        "\n",
        "def _sum_outer_products(data, a_inds, b_inds, weights=None):\n",
        "  Xab = data[a_inds] - data[b_inds]\n",
        "  if weights is not None:\n",
        "    return np.dot(Xab.T, Xab * weights[:, None])\n",
        "  return np.dot(Xab.T, Xab)"
      ],
      "metadata": {
        "id": "e2tKrEzLBy1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load hypergraph data from full raw arxiv data\n",
        "\n",
        "\n",
        "In this data there are some constraints, which are of higher order, which we need to keep track of while we are doing the classification task."
      ],
      "metadata": {
        "id": "KIans6ye9Z62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fullArxivWithOrcidAndCitations.csv', index_col=0, header=0, sep=',')\n",
        "\n",
        "df = df.sort_values(by='created')\n",
        "df['year'] = pd.DatetimeIndex(df['created']).year\n",
        "df = df[pd.DatetimeIndex(df['created']).year >= 1992]\n",
        "\n",
        "\n",
        "print('printing full dataset')\n",
        "\n",
        "df.head(30)"
      ],
      "metadata": {
        "id": "x73Jeisj9dbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification testing: with and without higher order constraints\n",
        "\n",
        "Now we want to test whether in the cases:\n",
        "1. with just second oder constraints, on similarities, e.g. set S, D and without higher order constraints\n",
        "\n",
        "2. with also higher order constraints from data, e.g. in sets S3, D3.\n",
        "\n",
        "we get any improvements in the classfication while using distance learning algorithms for the classification.\n",
        "\n",
        "We keep track of those constraints (S, D), or (S3, D3) while we are doing the classification tasks."
      ],
      "metadata": {
        "id": "G0uyovt-9sW5"
      }
    }
  ]
}